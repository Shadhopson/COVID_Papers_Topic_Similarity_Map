{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f76ff23c",
   "metadata": {},
   "source": [
    "## Load in Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc70dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "# most_valuable_edge Functions for Girvin-Newman\n",
    "from networkx.algorithms.centrality import edge_betweenness_centrality\n",
    "\n",
    "# Clustering Algorithms\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "from networkx.algorithms.community.quality import performance"
   ]
  },
  {
   "source": [
    "## Choose BERTopic or LDA\n",
    "You'll need to run this notebook twice to generate the output needed for the UI. First, you'll need to run with BERTopic inputs. This will generate one set of network graph-related tables. Then, you'll need to run it for LDA, which produces a different set of tables.\n",
    "\n",
    "You should utilize the variable in the next cell to make this selection. All other code should remain unchanged."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select BERTopic or LDA in this cell\n",
    "mode = \"BERTopic\"\n",
    "# mode = \"LDA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit this cell or others below\n",
    "if mode == \"BERTopic\":\n",
    "    # Input\n",
    "    source_doc_to_topic = \"02a_bert_string_doc_to_topic\"\n",
    "\n",
    "    # Output\n",
    "    topic_graph_table = \"03_bert_topic_graph\"\n",
    "    paper_graph_table = \"03_bert_paper_graph\"\n",
    "    topic_clust = \"03_bert_topic_clust_output\"\n",
    "    topic_pagerank = \"03_bert_topic_pagerank\"\n",
    "    paper_pagerank = \"03_bert_paper_pagerank\"\n",
    "    topic_clust_csv = \"03_bert_topic_clust_output.csv\"\n",
    "    topic_pagerank_csv = \"03_bert_topic_pagerank.csv\"\n",
    "    paper_pagerank_csv = \"03_bert_paper_pagerank.csv\"\n",
    "\n",
    "\n",
    "if mode == \"LDA\":\n",
    "    # Input\n",
    "    source_doc_to_topic = \"02b_lda_string_doc_to_topic\"\n",
    "\n",
    "    # Output\n",
    "    topic_graph_table = \"03_lda_topic_graph\"\n",
    "    paper_graph_table = \"03_lda_paper_graph\"\n",
    "    topic_clust = \"03_lda_topic_clust_output\"\n",
    "    topic_pagerank = \"03_lda_topic_pagerank\"\n",
    "    paper_pagerank = \"03_lda_paper_pagerank\"\n",
    "    topic_clust_csv = \"03_lda_topic_clust_output.csv\"\n",
    "    topic_pagerank_csv = \"03_lda_topic_pagerank.csv\"\n",
    "    paper_pagerank_csv = \"03_lda_paper_pagerank.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c60c7d",
   "metadata": {},
   "source": [
    "## Read in the Needed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d81b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MySQL Connection\n",
    "sqlEngine = create_engine('mysql+pymysql://root:p@ssw0rd1@cse6242_team094_mysqldb/cse6242_team094')\n",
    "dbConnection = sqlEngine.connect()\n",
    "\n",
    "# Read in the topic output\n",
    "df = pd.read_sql_table(source_doc_to_topic, con=dbConnection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af91f04f",
   "metadata": {},
   "source": [
    "## Perform some preprocessing on topic output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77161a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = np.array(df['cord_uid'])\n",
    "doc_topic = np.array(df['topic'])\n",
    "\n",
    "# Extract array of topic probabilities\n",
    "topic_prob = []\n",
    "for p in df['topic_prob']:\n",
    "    row = [float(r) for r in p.strip('\\n').replace(\n",
    "        '[', '').replace(']', '').split()]\n",
    "    topic_prob.append(row)\n",
    "\n",
    "topic_prob = np.array(topic_prob)\n",
    "\n",
    "# Array of Topic labels\n",
    "topic_id = ['Topic_' + str(i) for i in range(topic_prob.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a819179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays to dataframe\n",
    "df_topic_prob = pd.DataFrame(\n",
    "    data=topic_prob,\n",
    "    index=np.array(doc_id),\n",
    "    columns=topic_id)\n",
    "df_topic_prob.insert(0, 'topic_id', doc_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21105b6e",
   "metadata": {},
   "source": [
    "## Adjacency Matrix Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb491c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability threshold function\n",
    "def probThreshold(data, threshold: float = 0.01):\n",
    "    return np.where(data < threshold, 0, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc86a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity measure\n",
    "def simAbsCorr(data):\n",
    "    S = np.absolute(np.corrcoef(data))\n",
    "    return S\n",
    "\n",
    "\n",
    "def simSignedCorr(data):\n",
    "    S = (1 + np.corrcoef(data)) / 2\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc4130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacency functions\n",
    "def powerAdj(SimMat, Beta: int = 6):\n",
    "    A = SimMat ** Beta\n",
    "    np.fill_diagonal(A, 0)\n",
    "    return A\n",
    "\n",
    "\n",
    "def signumAdj(SimMat, tau: float = 0.0):\n",
    "    A = np.where(SimMat < tau, 0, 1)\n",
    "    np.fill_diagonal(A, 0)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71cb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topological Overlap Matrix function\n",
    "def TOMadjacency(AdjMat, threshold_quantile: float = 0.8):\n",
    "    '''\n",
    "    TOMadjacency calculates an adjacency matrix by the network overlap of nodes\n",
    "    in a weighted, undirected graph.\n",
    "    '''\n",
    "    # Calculate common neighbors of each node\n",
    "    L = AdjMat.dot(AdjMat.T)\n",
    "\n",
    "    # Calculate connectivity of node\n",
    "    Krow = AdjMat.sum(axis=1)\n",
    "    Kcol = AdjMat.sum(axis=0)\n",
    "    Kmin = np.array([np.minimum(k_i, Kcol) for k_i in Krow])\n",
    "\n",
    "    # Topological overlap\n",
    "    TOM = (L + AdjMat) / (Kmin + 1 - AdjMat)\n",
    "\n",
    "    TOM_filtered = np.where(\n",
    "        TOM >= np.quantile(\n",
    "            TOM, threshold_quantile), TOM, 0)\n",
    "\n",
    "    np.fill_diagonal(TOM_filtered, 0)\n",
    "\n",
    "    TOMlower = np.tril(TOM_filtered)\n",
    "\n",
    "    TOMsparse = csr_matrix(TOMlower)\n",
    "\n",
    "    return TOMsparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf12677",
   "metadata": {},
   "source": [
    "## Filter Out Bad Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d29ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data to exclude small probabilites\n",
    "thresh_val = 1 / topic_prob.shape[1]\n",
    "\n",
    "topic_prob_sigProbs = probThreshold(topic_prob, threshold=thresh_val)\n",
    "\n",
    "zeroTopic_doc = np.where(topic_prob_sigProbs.sum(axis=1) == 0)[0].tolist()\n",
    "\n",
    "doc_kept = np.delete(doc_id, zeroTopic_doc)\n",
    "\n",
    "topic_prob_filtered = np.delete(topic_prob_sigProbs, zeroTopic_doc, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1f67db",
   "metadata": {},
   "source": [
    "## Form and Save Topic Graph\n",
    "\n",
    "This will produce a rather small graph. With the sample 5000 papers, the graph's edge list will be ~67KB as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aced5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate adjacency of topics\n",
    "S_topic = simSignedCorr(topic_prob_filtered.T)\n",
    "\n",
    "A_topic = signumAdj(S_topic, tau=np.quantile(S_topic, 0.8))\n",
    "\n",
    "TOM_topic = TOMadjacency(A_topic, threshold_quantile=0.9)\n",
    "\n",
    "# Create graph\n",
    "topic_graph = nx.from_scipy_sparse_matrix(TOM_topic)\n",
    "\n",
    "# Assign graph node names to topic id's\n",
    "topic_label_mapping = dict(zip(topic_graph, topic_id))\n",
    "topic_graph = nx.relabel_nodes(topic_graph, topic_label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Edge List to Pandas\n",
    "topic_graph_df = nx.to_pandas_edgelist(topic_graph)\n",
    "\n",
    "# Insert the topic graph into MySQL\n",
    "topic_graph_df.to_sql(topic_graph_table, con=dbConnection, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535fdad1",
   "metadata": {},
   "source": [
    "## Form and Save Paper Graph\n",
    "\n",
    "This portion of the code is rather large and compute intensive. The resulting edge list, if saved as CSV, will be ~619MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a29aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate adjacency of papers\n",
    "S_paper = simAbsCorr(topic_prob_filtered)\n",
    "\n",
    "A_paper = powerAdj(S_paper, Beta=18)\n",
    "\n",
    "TOM_paper = TOMadjacency(A_paper, threshold_quantile=0.9)\n",
    "\n",
    "# Create graph\n",
    "paper_graph = nx.from_scipy_sparse_matrix(TOM_paper)\n",
    "\n",
    "# Assign graph node names to paper id's\n",
    "paper_label_mapping = dict(zip(paper_graph, doc_kept))\n",
    "paper_graph = nx.relabel_nodes(paper_graph, paper_label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Edge List to Pandas\n",
    "paper_graph_df = nx.to_pandas_edgelist(paper_graph)\n",
    "\n",
    "# Insert the paper graph into MySQL\n",
    "paper_graph_df.to_sql(paper_graph_table, con=dbConnection, if_exists='replace', chunksize=500)"
   ]
  },
  {
   "source": [
    "## Define a Helper Class for Integrating with NetworkX\n",
    "\n",
    "This class originally included additional methods, such as Kernighan Lin Bisection, Fluid Communities, and Greedy Modularity. However, these have been removed from the final code to keep only the needed portions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UndirectedDocumentGraph():\n",
    "    \"\"\"\n",
    "    This class will be used to form various graph representations of our document corpus.\n",
    "    The graph representation can be created all at once or incrementally.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create an empty, undirected NetworkX graph\n",
    "        self.nx_graph = nx.Graph()\n",
    "\n",
    "    ####################\n",
    "    # Graph Formation\n",
    "    ####################\n",
    "\n",
    "    def merge_graph_from_sparse_scipy(self, sp_matrix):\n",
    "        \"\"\"\n",
    "        Takes in a SciPy sparse matrix, representing our pair-wise document similarity, creates a new graph from\n",
    "        it, then merges with any existing nodes.\n",
    "        \"\"\"\n",
    "        # Load the new portion of the graph\n",
    "        new_graph = nx.from_scipy_sparse_matrix(\n",
    "            sp_matrix, parallel_edges=False, edge_attribute=\"weight\")\n",
    "\n",
    "        # An adjacency matrix will contain entries relating documents to themselves.\n",
    "        # These should be removed from the graph\n",
    "        new_graph.remove_edges_from(nx.selfloop_edges(new_graph))\n",
    "\n",
    "        # Merge it with the existing representation\n",
    "        self.nx_graph = nx.union(new_graph, self.nx_graph)\n",
    "\n",
    "    ####################\n",
    "    # Graph Similarity\n",
    "    ####################\n",
    "    def get_modularity(self, graph, communities):\n",
    "        \"\"\"\n",
    "        Calculates the modularity of a given partition of a graph. This will be one number for the whole partitioning.\n",
    "        NetworkX Doc:\n",
    "        https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.quality.modularity.html#networkx.algorithms.community.quality.modularity\n",
    "        \"\"\"\n",
    "        return modularity(graph, communities, weight=\"weight\")\n",
    "\n",
    "    def get_performance(self, graph, partition):\n",
    "        \"\"\"\n",
    "        The performance of a partition is the ratio of the number of intra-community edges plus\n",
    "        inter-community non-edges with the total number of potential edges\n",
    "        NetworkX Doc:\n",
    "        https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.quality.performance.html#networkx.algorithms.community.quality.performance\n",
    "        \"\"\"\n",
    "        return performance(graph, partition)\n",
    "\n",
    "    def girvan_newman(\n",
    "            self,\n",
    "            k: int,\n",
    "            most_valuable_edge: str = \"edge_betweenness_centrality\"):\n",
    "        \"\"\"\n",
    "        k - represents the number of tuples of communities from the algorithm\n",
    "        most_valuable_edge - function used to get the edge removed at each iteration\n",
    "        NetworkX Doc for the Girvan-Newman Method:\n",
    "        https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.centrality.girvan_newman.html#networkx.algorithms.community.centrality.girvan_newman\n",
    "        Helpful video explanation:\n",
    "        https://youtu.be/LtQoPEKKRYM\n",
    "        \"\"\"\n",
    "        if most_valuable_edge == \"edge_betweenness_centrality_equal_weight\":\n",
    "            # Default option for Girvan-Newman, assumes all edges of weight 1\n",
    "            comp = girvan_newman(self.nx_graph, edge_betweenness_centrality)\n",
    "        elif most_valuable_edge == \"edge_betweenness_centrality_equal_with_weight\":\n",
    "            # Take edge weight into account\n",
    "            def get_edge(G):\n",
    "                centrality = edge_betweenness_centrality(G, weight=\"weight\")\n",
    "                return max(centrality, key=centrality.get)\n",
    "            print(get_edge(self.nx_graph))\n",
    "            comp = girvan_newman(self.nx_graph, get_edge)\n",
    "        elif most_valuable_edge == \"least_similar\":\n",
    "            # Simple option of removing edge with least weight\n",
    "            def get_edge(G):\n",
    "                # Get edge based on the weight value (index 2 of triple)\n",
    "                u, v, w = min(G.edges(data=\"weight\"), key=itemgetter(2))\n",
    "                return (u, v)\n",
    "            comp = girvan_newman(self.nx_graph, get_edge)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid most_valuable_edge option for Girvan-Newman\")\n",
    "\n",
    "        # Create a list of dictionaries representing each row for the Pandas DF\n",
    "        node_dict_list = []\n",
    "\n",
    "        # Extract only the first specified number of communities and add them\n",
    "        # to the dictionary\n",
    "        num_communities = 2\n",
    "        for communities in itertools.islice(comp, k):\n",
    "            # Get the n number of communities\n",
    "            community_tuple = tuple(sorted(c) for c in communities)\n",
    "\n",
    "            # Calculate the modularity of the partitioning\n",
    "            mod = self.get_modularity(self.nx_graph, community_tuple)\n",
    "\n",
    "            # Calculate the performance of the partitioning\n",
    "            perf = self.get_performance(self.nx_graph, community_tuple)\n",
    "\n",
    "            # Loop through each of the communities\n",
    "            for cluster_id in range(len(community_tuple)):\n",
    "                # Get the list of nodes in the community\n",
    "                nodes = community_tuple[cluster_id]\n",
    "\n",
    "                # Loop through each of the nodes and form the Pandas DF row\n",
    "                # dictionary\n",
    "                for node in nodes:\n",
    "                    row_dict = {\n",
    "                        \"algorithm\": \"Girvan-Newman\",\n",
    "                        \"settings\": \"most_valuable_edge:\" + most_valuable_edge,\n",
    "                        \"num_clusters\": num_communities,\n",
    "                        \"cluster_id\": cluster_id,\n",
    "                        \"node_id\": node,\n",
    "                        \"modularity\": mod,\n",
    "                        \"performance\": perf\n",
    "                    }\n",
    "                    node_dict_list.append(row_dict)\n",
    "\n",
    "            # Increment the community count\n",
    "            num_communities += 1\n",
    "\n",
    "        # Create a Pandas DF from the results\n",
    "        clustering_df = pd.DataFrame(node_dict_list)\n",
    "\n",
    "        return clustering_df\n",
    "\n",
    "    def undirected_pagerank(self, alpha: float = 0.85, max_iter: int = 100):\n",
    "        \"\"\"\n",
    "        NetworkX Doc:\n",
    "        https://networkx.org/documentation/stable//reference/algorithms/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html\n",
    "        \"\"\"\n",
    "        # Get the dictionary output from the pagerank algorithm\n",
    "        pagerank_output = nx.pagerank(\n",
    "            self.nx_graph,\n",
    "            weight=\"weight\",\n",
    "            alpha=alpha,\n",
    "            max_iter=max_iter)\n",
    "\n",
    "        # Change the output to a Pandas DataFrame\n",
    "        node_id_list = []\n",
    "        page_rank_val_list = []\n",
    "        for node_id, page_rank_val in pagerank_output.items():\n",
    "            node_id_list.append(node_id)\n",
    "            page_rank_val_list.append(page_rank_val)\n",
    "\n",
    "        page_rank_df = pd.DataFrame.from_dict(data={\n",
    "            \"node_id\": node_id_list,\n",
    "            \"page_rank_val\": page_rank_val_list\n",
    "        })\n",
    "\n",
    "        # Sort by page rank value, descending\n",
    "        page_rank_df.sort_values(\n",
    "            by=['page_rank_val'],\n",
    "            ascending=False,\n",
    "            inplace=True)\n",
    "\n",
    "        return page_rank_df"
   ]
  },
  {
   "source": [
    "## Utilize the Helper Class for Community Detection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create topic graph using helper class\n",
    "topic_graph = UndirectedDocumentGraph()\n",
    "\n",
    "topic_graph.merge_graph_from_sparse_scipy(TOM_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paper graph using helper class\n",
    "paper_graph = UndirectedDocumentGraph()\n",
    "\n",
    "paper_graph.merge_graph_from_sparse_scipy(TOM_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Girvan-Newman (edge_betweenness_centrality_equal_with_weight):\")\n",
    "\n",
    "girvan_newman_df_ebcw = topic_graph.girvan_newman(20, \"edge_betweenness_centrality_equal_with_weight\")\n",
    "\n",
    "girvan_newman_df_ebcw_diag = girvan_newman_df_ebcw[[\n",
    "    'num_clusters', 'modularity', 'performance']].drop_duplicates(ignore_index=True)\n",
    "\n",
    "print(girvan_newman_df_ebcw_diag, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the clustering of topics to MySQL\n",
    "girvan_newman_df_ebcw.iloc[:, 2:].to_sql(topic_clust, con=dbConnection, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the clustering of topics to CSV\n",
    "output_path = topic_clust_csv\n",
    "girvan_newman_df_ebcw.iloc[:, 2:].to_csv(output_path, sep=\"|\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get graph page ranks\n",
    "topic_pagerank_df = topic_graph.undirected_pagerank()\n",
    "paper_pagerank_df = paper_graph.undirected_pagerank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pagerank lists to MySQL\n",
    "topic_pagerank_df.to_sql(topic_pagerank, con=dbConnection, if_exists='replace')\n",
    "\n",
    "paper_pagerank_df.to_sql(paper_pagerank, con=dbConnection, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pagerank lists to CSV\n",
    "topic_pagerank_df.to_csv(topic_pagerank_csv, sep=\"|\", header=True, index=False)\n",
    "\n",
    "paper_pagerank_df.to_csv(paper_pagerank_csv, sep=\"|\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics plot, modularity and performance\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "fig.suptitle(\n",
    "    'Girvan-Newman (edge_betweenness_centrality_equal_with_weight): 5k sample')\n",
    "ax1.plot(\n",
    "    girvan_newman_df_ebcw_diag['num_clusters'],\n",
    "    girvan_newman_df_ebcw_diag['modularity'],\n",
    "    '.-')\n",
    "ax1.axvline(x=6, color='red')\n",
    "ax1.set_ylabel('Modularity')\n",
    "ax2.plot(\n",
    "    girvan_newman_df_ebcw_diag['num_clusters'],\n",
    "    girvan_newman_df_ebcw_diag['performance'],\n",
    "    '.-',\n",
    "    color='green')\n",
    "ax2.axvline(x=6, color='red')\n",
    "ax2.set_ylabel('Performance')\n",
    "ax2.set_xlabel('# Clusters')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}