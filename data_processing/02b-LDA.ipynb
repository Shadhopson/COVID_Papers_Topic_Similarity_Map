{
 "cells": [
  {
   "source": [
    "## Load in Required Packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pymysql"
   ]
  },
  {
   "source": [
    "## Choose AWS or Local\n",
    "\n",
    "This code does some initial setup depending upon if you're running this code in AWS EC2 or locally via Docker containers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit these 2 lines\n",
    "LOCAL_RUN_MODE = \"LOCAL\"\n",
    "AWS_EC2_RUN_MODE = \"AWS_EC2\"\n",
    "\n",
    "# Uncomment the line you want to use, matching to where you're running the code\n",
    "run_mode = LOCAL_RUN_MODE\n",
    "# run_mode = AWS_EC2_RUN_MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (run_mode == LOCAL_RUN_MODE):\n",
    "    # Configure MySQL Connection\n",
    "    sqlEngine = create_engine('mysql+pymysql://root:p@ssw0rd1@cse6242_team094_mysqldb/cse6242_team094')\n",
    "    dbConnection = sqlEngine.connect()\n",
    "    \n",
    "    table = pd.read_sql_table(\"processed_abstracts\", con=dbConnection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (run_mode == AWS_EC2_RUN_MODE):\n",
    "    # Read the data from the Parquet files on the EC2 instance\n",
    "    table = pd.read_parquet(\"abstract_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.dropna(inplace=True)\n",
    "table.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "source": [
    "## Transform the Preprocessed Abstracts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.rename({\"abstract_tokens\":\"abstract\", \"abstract\":\"abstract_raw\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = table[\"abstract\"]\n",
    "vectored = TfidfVectorizer(max_features=2**12)\n",
    "X = vectored.fit_transform(text)\n",
    "print(\"vectors formed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_reduced = pca.fit_transform(X.toarray())\n",
    "print(\"pca fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(n_components=k,  \n",
    "                     random_state=42,\n",
    "                     init_params='kmeans',\n",
    "                     covariance_type = \"spherical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.fit(X_reduced)\n",
    "print(\"Gaussian Fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gm.predict(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gm_proba = gm.predict_proba(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"probability of topic gained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['topic'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = pd.DataFrame([x for x in y_gm_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df = pd.concat([table[[\"cord_uid\",\"topic\"]],prob_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (run_mode == LOCAL_RUN_MODE):\n",
    "    # Write to MySQL\n",
    "    doc_topic_df.to_sql(\"02b_lda_doc_to_topic\", con=dbConnection, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (run_mode == AWS_EC2_RUN_MODE):\n",
    "    # Save to a CSV\n",
    "    doc_topic_df.to_csv(\"lda_doc_to_topic.csv\")\n",
    "\n",
    "    print(\"saved lda_doc_to_topic.csv\")\n",
    "    print(doc_topic_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_prob = [str(x) for x in y_gm_proba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table[\"topic_prob\"] = str_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (run_mode == LOCAL_RUN_MODE):\n",
    "    # Write to MySQL\n",
    "    table[[\"cord_uid\",\"topic\",\"topic_prob\"]].to_sql(\"02b_lda_string_doc_to_topic\", con=dbConnection, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (run_mode == AWS_EC2_RUN_MODE):\n",
    "    # Save to a CSV\n",
    "    table[[\"cord_uid\",\"topic\",\"topic_prob\"]].to_csv(\"string_lda_doc_to_topic.csv\")\n",
    "\n",
    "    print(\"saved string_lda_doc_to_topic.csv\")\n",
    "    print(table[[\"cord_uid\",\"topic\",\"topic_prob\"]].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize our data in each cluster\n",
    "vectorizers = []\n",
    "    \n",
    "for ii in range(0, 20):\n",
    "    vectorizers.append(CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = []\n",
    "print(\"cvec\")\n",
    "for current_cluster, cvec in enumerate(vectorizers):\n",
    "        vectorized_data.append(cvec.fit_transform(table.loc[table['topic'] == current_cluster, 'abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS_PER_CLUSTER = 10\n",
    "lda_models = []\n",
    "for ii in range(0, 20):\n",
    "    # Latent Dirichlet Allocation Model\n",
    "    lda = LatentDirichletAllocation(n_components=TOPICS_PER_CLUSTER, max_iter=10, learning_method='online', verbose=False, random_state=42)\n",
    "    lda_models.append(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_lda_data = []\n",
    "print(\"fitting lda\")\n",
    "for current_cluster, lda in enumerate(lda_models):\n",
    "    # print(\"Current Cluster: \" + str(current_cluster))\n",
    "    \n",
    "    if vectorized_data[current_cluster] != None:\n",
    "        clusters_lda_data.append((lda.fit_transform(vectorized_data[current_cluster])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for printing keywords for each topic\n",
    "def selected_topics(model, vectorizer, top_n=3):\n",
    "    current_words = []\n",
    "    keywords = []\n",
    "    \n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        words = [(vectorizer.get_feature_names()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        #print(words)\n",
    "        #break\n",
    "        for word in words:\n",
    "            if word[0] not in current_words:\n",
    "                keywords.append(word)\n",
    "                current_words.append(word[0])\n",
    "        #print(keywords)\n",
    "        #print(current_words)\n",
    "        \n",
    "    keywords.sort(key = lambda x: x[1])  \n",
    "    keywords.reverse()\n",
    "    return_values = []\n",
    "    for ii in keywords:\n",
    "        return_values.append(ii)\n",
    "    return return_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = []\n",
    "print(\"getting topic words\")\n",
    "for current_vectorizer, lda in enumerate(lda_models):\n",
    "    # print(\"Current Cluster: \" + str(current_vectorizer))\n",
    "    if vectorized_data[current_vectorizer] != None:\n",
    "        all_keywords.append(selected_topics(lda, vectorizers[current_vectorizer]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topic_words = [x[:10] for x in all_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_per_topic = pd.DataFrame({\"Topic\":[x for x in range(20)],\"related_words\":top_topic_words})\n",
    "word_per_topic[\"related_words\"] = word_per_topic[\"related_words\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (run_mode == LOCAL_RUN_MODE):\n",
    "    # Write to MySQL\n",
    "    word_per_topic.to_sql(\"02b_lda_string_topic_to_words\", con=dbConnection, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (run_mode == AWS_EC2_RUN_MODE):\n",
    "    # Save to a CSV\n",
    "    word_per_topic.to_csv(\"lda_string_topic_to_words.csv\")\n",
    "\n",
    "    print(\"making lda_string_topic_to_words.csv\")\n",
    "    print(word_per_topic.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = top_topic_words\n",
    "flat_probs = []\n",
    "for words in probs:\n",
    "    doc_words = []\n",
    "    for word in words:\n",
    "        doc_words.append(word[0])\n",
    "        doc_words.append(word[1])\n",
    "    flat_probs.append(doc_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_probs_df = pd.DataFrame(flat_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_topic_df = pd.concat([word_per_topic[\"Topic\"],flat_probs_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (run_mode == LOCAL_RUN_MODE):\n",
    "    # Write to MySQL\n",
    "    extended_topic_df.to_sql(\"02b_lda_topic_to_words\", con=dbConnection, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (run_mode == AWS_EC2_RUN_MODE):\n",
    "    # Save to a CSV\n",
    "    extended_topic_df.to_csv(\"lda_topic_to_words.csv\")\n",
    "\n",
    "    print(\"making lda_topic_to_words.csv\")\n",
    "    print(extended_topic_df.tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}