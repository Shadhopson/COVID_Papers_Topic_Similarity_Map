{
 "cells": [
  {
   "source": [
    "## Choose AWS or Local\n",
    "This code does some initial setup depending upon if you're running this code in AWS EMR or locally via Docker Containers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit these 2 lines\n",
    "LOCAL_RUN_MODE = \"LOCAL\"\n",
    "AWS_EMR_RUN_MODE = \"AWS_EMR\"\n",
    "\n",
    "# Uncomment the line you want to use, matching to where you're running the code\n",
    "run_mode = LOCAL_RUN_MODE\n",
    "# run_mode = AWS_EMR_RUN_MODE\n",
    "\n",
    "# Define AWS bucket (adjust if S3 bucket name is different than the readme instructions)\n",
    "bucket = \"s3://cse6242-team094-spring2021\"\n",
    "\n",
    "# For AWS, specify which file you'd like to run (sample or full dataset, sample recommended for speed)\n",
    "csv_file = 'metadata_sample_5000.csv' # Sample dataset\n",
    "#csv_file = 'metadata.csv' # Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (run_mode == LOCAL_RUN_MODE):\n",
    "    # Initialize the SparkSession\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = (SparkSession.builder.master(\"local\")\n",
    "                                 .config(\"spark.jars\", \"/usr/share/java/mysql-connector-java-8.0.23.jar\")\n",
    "                                 .getOrCreate())\n",
    "    spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, lower, col, udf, posexplode, collect_list, concat_ws\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField\n",
    "from pyspark.sql import *\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "import nltk\n",
    "import contractions\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve additional artifacts needed for NLTK\n",
    "if (run_mode == LOCAL_RUN_MODE):\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "    nltk.download(\"stopwords\")\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"omw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to load in data, write results to parquet, and import stopword data as PySpark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(filename, path):\n",
    "    # Load Raw Data \n",
    "    raw_data = (spark.read\n",
    "        .option(\"quote\", \"\\\"\")\n",
    "        .option(\"escape\", \"\\\"\")\n",
    "        .csv(path + filename, header=True, inferSchema=True))\n",
    "    print(\"Number of papers: \",raw_data.count()) # Prints # of papers (# of records, as each record is one paper)\n",
    "    return raw_data\n",
    "\n",
    "def write_to_parquet(path, subfolder):\n",
    "    abstracts.write.parquet(path+subfolder)\n",
    "\n",
    "# Define list of stopwords from nltk library, make additions appropriate to this project, then convert into spark dataframe\n",
    "stopwords_var = stopwords.words('english')\n",
    "stopwords_var.extend(['background','objective','introduction','abstract','conclusion','figure','table','chart'])\n",
    "stopwords_df = spark.createDataFrame(stopwords_var, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data, print number of total papers, and preview first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (run_mode == LOCAL_RUN_MODE):\n",
    "    raw_data = load_csv_data(filename='metadata_sample_5000.csv', path=\"/home/jovyan/sample_data/\")\n",
    "else:\n",
    "    raw_data = load_csv_data(filename=csv_file, path=bucket + \"/read/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicate rows based on repeated cord_uid or repeated abstract text, and drop rows with null values.\n",
    "Titles, authors, and other features are allowed to be repeated (it's possible that two papers are titled identically by coincidence, but it is less likely that two papers have the same exact abstract text and impossible for them to have the same cord_uid). The number of resulting papers after eliminating duplicates and null rows is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.dropDuplicates(['cord_uid']) \\\n",
    "    .dropDuplicates(['abstract']) \\\n",
    "    .na.drop(subset=['cord_uid','title','authors','abstract'])\n",
    "print(\"Number of papers: \",raw_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create three new dataframes for each of the relevant datasets (abstract text, titles, and metadata).\n",
    "- Abstracts: Includes cord_uid, raw abstract text, pre-processed abstract text.\n",
    "- Titles: Includes cord_uid, title text.\n",
    "- Metadata: Includes cord_uid, author names, publication date, publishing journal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_raw = raw_data.select(['cord_uid','abstract'])\n",
    "titles = raw_data.select(['cord_uid','title'])\n",
    "metadata = raw_data.select(['cord_uid','authors','publish_time','journal', 'url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin to pre-process abstract text data. Following steps are taken in pre-processing:\n",
    "1. Convert all letters to lowercase.\n",
    "2. Expand contractions with a user-defined-function.\n",
    "3. Replace all non-alpha characters with spaces using regular expressions.\n",
    "    (spaces were used instead of deletion because many forms of punctuation can be used to separate words, like hyphens, slashes, etc. Additionally, lemmatization isn't strong enough to replace the possessive format of unfamiliar words, so using deletion could result in two terms instead of one for many instances. For example, mRNA's would be replaced with mRNAs if the apostrophe is simply deleted. If using a space instead of delection, the mRNA's will eventually be transformed to mRNA as the base term, which will more accurately track the topic. Other forms of punctuation aren't impacted with spaces instead of deletion because the tokenization will eliminate white spaces later.\n",
    "4. Remove all single character words using regular expressions.\n",
    "5. Tokenize all long strings of texts into individual words called tokens (with a UDF: user defined function).\n",
    "6. Flatten the data to eliminate the column of lists of tuples (using PySpark's explode function).\n",
    "7. Lemmatize all tokens to the respective base terms (with a UDF: user defined function).\n",
    "8. Compare lemmatized tokens to list of stop words and filter accordingly (using left-anti join).\n",
    "9. Join pre-processed tokens into comma seperated strings for each respective paper to store data in SQL database.\n",
    "10. Join raw abstract text rdd with abstract tokens rdd.\n",
    "11. Store joined rdd as parquet outputs.\n",
    "\n",
    "An effort was made to merge the steps above where appropriate, to avoid having too many independent operations that would be otherwise computationally intensive. To begin the pre-processing, all of the user-defined functions will be defined. Afterwards, the various user-defined functions and pre-processing operations will be completed on the PySpark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF-1: User defined function to expand contractions. Takes in a string and returns the expanded format. E.g. can't --> can not\n",
    "contractions_udf = udf(lambda z: contractions.fix(z), StringType())\n",
    "\n",
    "\n",
    "# UDF-2: Function to tokenize words and assign part of speech tags to tokens using nltk library.\n",
    "def nltk_pos_tag(z):\n",
    "    x = nltk.word_tokenize(z)\n",
    "    y = nltk.pos_tag(x)\n",
    "    return y\n",
    "# Create PySpark UDF from previously written function\n",
    "pos_tag_udf = udf(lambda z: nltk_pos_tag(z), ArrayType(ArrayType(StringType())))\n",
    "\n",
    "\n",
    "# UDF-3: User defined function to convert nltk pos tags to wordnet pos tags. Then takes the word and it's pos tag to lemmatize it.\n",
    "\n",
    "def wn_lemma(tokens_postag):\n",
    "    from nltk.corpus import wordnet, stopwords\n",
    "    # Define list of stopwords from nltk library, make additions appropriate to this project, then convert into spark dataframe\n",
    "    stopwords_var = stopwords.words('english')\n",
    "    stopwords_var.extend(['background','objective','introduction','abstract','conclusion','figure','table','chart'])\n",
    "\n",
    "    # Adjust output tags from nltk.pos_tag to appropriate format for WordNet Lemmatizer input\n",
    "    wn_lemmatizer = nltk.WordNetLemmatizer()\n",
    "    if tokens_postag[1].startswith('J'):\n",
    "        wn_tag = wordnet.ADJ\n",
    "    elif tokens_postag[1].startswith('V'):\n",
    "        wn_tag = wordnet.VERB\n",
    "    elif tokens_postag[1].startswith('N'):\n",
    "        wn_tag = wordnet.NOUN\n",
    "    elif tokens_postag[1].startswith('R'):\n",
    "        wn_tag = wordnet.ADV\n",
    "    else:\n",
    "        wn_tag = None\n",
    "    \n",
    "    if wn_tag is not None:\n",
    "        lemma = wn_lemmatizer.lemmatize(tokens_postag[0], pos=wn_tag)\n",
    "    else:\n",
    "        lemma = tokens_postag[0]\n",
    "    return lemma\n",
    "\n",
    "lemmatize_udf = udf(lambda z: wn_lemma(z), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps 1-3: The operation below converts the abstract text to lowercase, expands contractions, and then replaces all non-alpha characters with spaces\n",
    "abstracts = abstracts_raw.select('cord_uid', regexp_replace(contractions_udf(lower(col(\"abstract\"))), \"[^a-zA-Z\\s]\", \" \").alias(\"abstract_proc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Eliminate all single character words\n",
    "abstracts = abstracts.withColumn('abstract_proc', regexp_replace(col(\"abstract_proc\"), \"\\s(\\w\\s)+\", \" \")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create tokens and part of speech tags\n",
    "abstracts = abstracts.select(\"cord_uid\", pos_tag_udf(col(\"abstract_proc\")).alias(\"abstract_tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Flatten the list of tuples - [(token1, POS_tag1), (token2, POS_tag2), ...] (posexplode does this while creating indices to preserve order)\n",
    "abstracts = abstracts.select(\"cord_uid\", posexplode(col(\"abstract_tokens\")).alias(\"pos\",\"abstract_tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Lemmatize all the tokens to their base terms\n",
    "abstracts = abstracts.withColumn(\"abstract_tokens\", lemmatize_udf(col(\"abstract_tokens\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Eliminate rows with tokens that are stopwords (using a left anti-join)\n",
    "abstracts = abstracts.join(stopwords_df,abstracts.abstract_tokens ==  stopwords_df.value,\"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Collapse results back into a string of space separated values for tokens. Use the previously created indexes to preserve order of tokens\n",
    "abstracts = abstracts.orderBy(\"cord_uid\",\"pos\").groupBy(\"cord_uid\").agg(concat_ws(\" \", collect_list(\"abstract_tokens\")).alias(\"abstract_tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Join abstracts_raw rdd with abstracts rdd\n",
    "abstracts_final = abstracts_raw.join(abstracts, abstracts_raw.cord_uid == abstracts.cord_uid, \"left\") \\\n",
    "    .drop(abstracts.cord_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This saving code will be executed when running in the AWS EMR cluster\n",
    "if (run_mode == AWS_EMR_RUN_MODE):\n",
    "    # Step 11: Write output to parquet files\n",
    "    write_bucket = bucket + \"/write/\"\n",
    "    abstracts_final.write.parquet(write_bucket+\"abstract_parquet\")\n",
    "    titles.write.parquet(write_bucket+\"title_parquet\")\n",
    "    metadata.write.parquet(write_bucket+\"metadata_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This saving code will be executed when running a local, Docker Container demo\n",
    "if (run_mode == LOCAL_RUN_MODE):      \n",
    "    (abstracts_final.write\n",
    "                    .format('jdbc')\n",
    "                    .options(url=\"jdbc:mysql://cse6242_team094_mysqldb/cse6242_team094?sessionVariables=sql_mode='NO_ENGINE_SUBSTITUTION'&jdbcCompliantTruncation=false\",\n",
    "                            driver='com.mysql.jdbc.Driver',\n",
    "                            dbtable='processed_abstracts',\n",
    "                            user='root',\n",
    "                            password='p@ssw0rd1',\n",
    "                            createTableColumnTypes='abstract VARCHAR(65536), cord_uid VARCHAR(1024), abstract_tokens VARCHAR(65536)')\n",
    "                    .mode('overwrite')\n",
    "                    .save())\n",
    "    (titles.write\n",
    "           .format('jdbc')\n",
    "           .options(url='jdbc:mysql://cse6242_team094_mysqldb/cse6242_team094',\n",
    "                    driver='com.mysql.jdbc.Driver',\n",
    "                    dbtable='titles',\n",
    "                    user='root',\n",
    "                    password='p@ssw0rd1')\n",
    "           .mode('overwrite')\n",
    "           .save())\n",
    "    (metadata.write\n",
    "             .format('jdbc')\n",
    "             .options(url='jdbc:mysql://cse6242_team094_mysqldb/cse6242_team094',\n",
    "                      driver='com.mysql.jdbc.Driver',\n",
    "                      dbtable='metadata',\n",
    "                      user='root',\n",
    "                      password='p@ssw0rd1')\n",
    "              .mode('overwrite')\n",
    "              .save())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}